---
title: "wk10-1_k-인접기법"
author: "choi"
date: '2020 11 12 '
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                       warning = FALSE, message = FALSE)
```


# 1.분류
- 분류- 지도학습. 타겟범주를 알고 있는 데이터로 분류규칙을 생성하고 새로운 데이터를 특정범주에 분류하는 기법  
- 군집화- 비지도학습. 독립변수들의 속성을 기반으로 객채들을 그룹화하는 방법  

# 2.k-인접기법
k-인접방법(kNN): k개의 가장 가까운 이웃들을 사용해서 분류하는 방법  
- 최적 k는?
 - k가 너무 크면 데이터 구조를 파악하기 어렵고, 너무 작으면 과적합위험이 있음  
 - 교차검증으로 정확도가 높은 k를 선정
```
장점  
- 단순하며 효율적
- 데이터 분산을 추정할 필요 없음
- 빠른 훈련 단계
```

```
단점
- 모델을 생성하지 않음
- 느린 분류 단계
- 많은 메모리 필요 
- 결측치는 추가 적업 필요
```

- kNN 을 수행하기 위한 추가 패키지 설치

```{r}
# packages
#install.packages("class")#kNN 수행을 위한 패키지
#install.packages("gmodels")#분류분석 후 검증에 사용되는 cross table을 위한 패키지
#install.packages("scales")#최적 k 등 그래프를 위한 패키지
library(class)
library(gmodels)
library(scales)
```


```{r}
# set working directory
setwd("C:\\Users\\user\\Desktop\\class_R\\data mining\\data")
```

```
데이터 불러들이기
```
```{r}
# read csv file
iris<-read.csv(file.choose())
# head(iris)
# str(iris)
attach(iris)
```
```
데이터분할(학습데이터 2/3, 검증데이터1/3)
```

```{r}
# training/ test data : n=150
set.seed(1000)
N=nrow(iris)
tr.idx=sample(1:N, size=N*2/3, replace=FALSE)
```

```
iris.train(독립변수4개를 포함한 100개의 데이터)
iris.test(독립변수4개를 포함한 50개의 데이터)

trainLabels(학습데이터의 타겟변수)
testLabels(검증데이터의 타겟변수)
```

```{r}
# attributes in training and test
iris.train<-iris[tr.idx,-5]
iris.test<-iris[-tr.idx,-5]
# target value in training and test
trainLabels<-iris[tr.idx,5]
testLabels<-iris[-tr.idx,5]

train<-iris[tr.idx,]
test<-iris[-tr.idx,]
```

- kNN함수 : knn(train=학습데이터, test=검증데이터, cl=타겟변수, k=)

```{r}
# knn (5-nearest neighbor)
md1<-knn(train=iris.train,test=iris.test,cl=trainLabels,k=5)
md1
help(knn) #knn의 매뉴얼

```

```{r}
# accuracy of 5-nearest neighbor classification
CrossTable(x=testLabels,y=md1, prop.chisq=FALSE)
help(CrossTable)
```
```
- 정확도 : 47/50 -> 94%
- versicolor를 virginica로 오분류(2개)
- virginica를 versicolor로 오분류(1개)
- 오분류율 : 3/50 -> 6%
```

# wk10-2_k-인접기법

## 6.kNN에서 최적 k 탐색
- 최적 k의 탐색 : 1 to nrow(train_data)/2 (여기서는 1 to 50 까지)



```{r}
# optimal k selection (1 to n/2)
accuracy_k <- NULL
# try k=1 to nrow(train)/2, may use nrow(train)/3(or 4,5) depending the size of n in train data
nnum<-nrow(iris.train)/2
for(kk in c(1:nnum))
{
  set.seed(1234)
  knn_k<-knn(train=iris.train,test=iris.test,cl=trainLabels,k=kk)
  accuracy_k<-c(accuracy_k,sum(knn_k==testLabels)/length(testLabels))
}

# plot for k=(1 to n/2) and accuracy
test_k<-data.frame(k=c(1:nnum), accuracy=accuracy_k[c(1:nnum)])
plot(formula=accuracy~k, data=test_k,type="o",ylim=c(0.5,1), pch=20, col=3, main="validation-optimal k")
with(test_k,text(accuracy~k,labels = k,pos=1,cex=0.7))
```

```{r}
# minimum k for the highest accuracy
min(test_k[test_k$accuracy %in% max(accuracy_k),"k"])
```

k = 7에서 정확도(.98)가 가장 높음

- 최종 kNN모형 (k=7)


```{r}
#k=7 knn
md1<-knn(train=iris.train,test=iris.test,cl=trainLabels,k=7)
CrossTable(x=testLabels,y=md1, prop.chisq=FALSE)
```

```
- 정확도 : 49/50 -> 98%
- versicolor를 virginica로 오분류(1개)
- 오분류율 : 1/50 ->2%
```

## 6.kNN(k=7)의 결과-그래픽

```{r}
# graphic display
plot(formula=Petal.Length ~ Petal.Width,
     data=iris.train,col=alpha(c("purple","blue","green"),0.7)[trainLabels],
     main="knn(k=7)")
points(formula = Petal.Length~Petal.Width,
       data=iris.test,
       pch = 17,
       cex= 1.2,
       col=alpha(c("purple","blue","green"),0.7)[md1]
)
legend("bottomright",
       c(paste("train",levels(trainLabels)),paste("test",levels(testLabels))),
       pch=c(rep(1,3),rep(17,3)),
       col=c(rep(alpha(c("purple","blue","green"),0.7),2)),
       cex=0.9
)
```

- petal.width와 petal.length에 산점도를 그려보면 setosa는 잘 분류됨.
- virginica와 versicolor는 분류가 잘 되지 않음.

## 7.Weighted kNN
- 거리에 따라 가중치를 부여하는 두 가지 알고리즘이 존재 (패키지 : kknn)


```{r}
## Weighted KNN packages
install.packages("kknn")#weighted value knn
library(kknn)
help("kknn")
```

## 7.Weighted kNN 결과

- k=5, distance=1
```{r}
# weighted knn
md2<-kknn(Species~., train=train,test=iris.test,k=5,distance=1,kernel="triangular")
md2
```

- weighted kNN의 결과를 md2로 저장
- weighted kNN의 결과를 보기 위해서 fitted함수를 이용(fitted(md2))

```{r}
# to see results for weighted knn
md2_fit<-fitted(md2)
md2_fit
```


```{r}
# accuracy of weighted knn
CrossTable(x=testLabels,y=md2_fit,prop.chisq=FALSE,prop.c=FALSE)
```

- 정확도 : 47/50 -> 94%
- versicolor를 virginica로 오분류(2개)
- virginica를 versicolor로 오분류(1개)

- k =7, distance=2로 옵션 변경한 결과

```{r}
# weighted knn (k=7, distance=2)
md3<-kknn(Species~., train=train,test=iris.test,k=7,distance=2,kernel="triangular")
md3
# to see results for weighted knn
md3_fit<-fitted(md3)
md3_fit
# accuracy of weighted knn
CrossTable(x=testLabels,y=md3_fit,prop.chisq=FALSE,prop.c=FALSE)
```

- 정확도 : 49/50 -> 98%
- virginica를 versicolor로 오분류(1개)

