## 03 LDA(Linear Discriminant Analysis)

### LDA개요
LDA(Linear Discriminant Analysis)는 선형 판별 분석법으로 불리며, PCA와 매우 유사합니다. LDA는 PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법이지만, 중요한 차이는 LDA는 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소합니다. PCA는 입력 데이터의 변동성의 가장 큰 축을 찾았지만, LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾습니다.

LDA는 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산과 클래스 내부 분산의 비율을 최대화 하는 방식으로 차원을 축소합니다. 즉, 클래스 간 분산은 최대한 크게 가져가고, 클래스 내부의 분산은 최대한 작게 가져가는 방식입니다. 다음 그림은 좋은 클래스 분리를 위해 클래스 간 분산이 크고 클래스 내부의 분산이 작은 것을 표현한 것입니다.



일반적으로 LDA를 구하는 스텝은 PCA와 유사하난 가장 큰 차이점은 공분산 행렬이 아니라 위에 설명한 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤, 이 행렬에 기반해 고유백터를 구하고 입력 데이터를 투영한다는 점입니다. LDA를 구하는 스텝은 다음과 같습니다.

 1. 클래스 내부와 클래스 간 분산 행렬을 구합니다. 이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 백터를 기반으로 구합니다.

 2. 클래스 내부 분산은 행렬을 SW, 클래스 간 분산 행렬을 SB라고 하면 다음 식으로 두 행렬을 고유백터로 분해할 수 있습니다.

 3. 고유값이 가장 큰 순으로 K개(LDA변환 차수만큼) 추출합니다.

 4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환합니다.

### 붓꽃 데이터 세트에 LDA 적용하기

붓꽃 데이터 세트를 사이킷런의 LDA를 이용해 변환하고, 그 결과를 품종별로 시각화해 보겠습니다.
사이킷런은 LDA를 LinearDiscriminantAnalysis 클래스로 제공합니다. 붓꽃 데이터 세트를 로드하고 표준 정규 분포로 스케일링합니다.


```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris


iris = load_iris()
iris_scaled = StandardScaler().fit_transform(iris.data)
```

2개의 컴포넌트로 붓꽃 데이터를 LDA 변환하겠습니다. PCA와 다르게 LDA에서 한 가지 유의해야 할점은 LDA는 실제로는 PCA와 다르게 비지도학습이 아닌 지도학습이라는 것입니다. 즉, 클래스의 결정값이 변환 시에 필요합니다. 다음 lda 객체의 fit() 메서드를 호출할 때 결정값이 입력됐음에 유의하세요.


```python
lda = LinearDiscriminantAnalysis(n_components=2)
lda.fit(iris_scaled, iris.target)
iris_lda = lda.transform(iris_scaled)
print(iris_lda.shape)
```

    (150, 2)
    

이제 LDA 변환된 입력 데이터 값을 2차원 평면에 품종별로 표현해 보겠습니다. 소스 코드는 앞의 PCA 예제와 큰 차이가 없습니다.


```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

lda_columns=['lda_component_1', 'lda_component_2']
irisDF_lda = pd.DataFrame(iris_lda, columns=lda_columns)
irisDF_lda['target']=iris.target

#setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현
markers=['^', 's', 'o']

#setosa의 target 값은 0, versicolor는 1, virginica는 2, 각 target별로 다른 모양으로 산점도로 표시
for i, marker in enumerate(markers):
  x_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_1']
  y_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_2']

  plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])

plt.legend(loc='upper right')
plt.xlabel('lda_component_1')
plt.ylabel('lda_component_2')
plt.show()
```


![png](output_8_0.png)


## 04 SVD(Singular Value Decomposition)

### SVD 개요
SVD 역시 PCA와 유사한 행렬 분해 기법을 이용합니다. PCA의 경우 정방행렬(즉, 행과 열의 크기가 같은 행렬)만을 고유벡터로 분해할 수 있지만, SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있습니다. 일반적으로 SVD는 m X n 크기의 행렬 A를 다음과 같이 분해하는 것을 의미합니다.

SVD는 특이값 분해로 불리며, 행렬 U와 V에 속한 백터는 특이벡터이며, 모든 특이벡터는 서로 직교하는 성질을 가집니다. Σ는 대각행렬이며, 행렬의 대각에 위치한 값만 0이 아니고 나머지 위치의 값은 모두 0입니다. Σ이 위치한 0이 아닌 값이 바로 행렬 A의 특이값입니다. SVD는 A의 차원이 m X n 일 때 U의 차원이 m X m, Σ의 차원이 m X n, V의 차원이 n X n으로 분해합니다.  



하지만 일반적으로는 다음과 같이 Σ의 비대각인 부분과 대각원소 중에 특이값이 0인 부분도 모두 제거하고 제거된Σ에 대응되는 U와 V 원소도 함께 제거해 차원을 줄인 형태로 SVD를 적용합니다. 이렇게 컴팩트한 형태로 SVD를 적용하면 A의 차원이 m X n일 때, U의 차원을 m X p, Σ의 차원을 p X p, V의 차원을 p X n으로 분해 합니다.



Truncated SVD는 Σ의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응하는 U와 V의 원소도 함께 제거해 더욱 차원을 줄인 형태로 분해하는 것입니다. 일반적인 SVD는 보통 넘파이나 사이파이 라이브러리를 이용해 수행합니다. 넘파이의 SVD를 이용해 SVD 연산을 수행하고, SVD로 분해가 어떤 식으로 되는지 간단한 예제를 통해 살펴보겠습니다.

새로운 주피터 노트불을 생성하고 넘파이의 SVD 모듈인 numpy.linalg.svd를 로딩합니다. 그리고 랜덤한 4 X 4 넘파이 행렬을 생성합니다. 랜덤 행렬을 생성하는 이유는 행렬의 개별 로우끼리의 의존성을 없애기 위해서입니다.


```python
# 넘파이의 svd 모듈 임포트
import numpy as np
from numpy.linalg import svd

# 4X4 랜덤 행렬 a 생성
np.random.seed(121)
a = np.random.randn(4, 4)
print(np.round(a, 3))
```

    [[-0.212 -0.285 -0.574 -0.44 ]
     [-0.33   1.184  1.615  0.367]
     [-0.014  0.63   1.71  -1.327]
     [ 0.402 -0.191  1.404 -1.969]]
    

이렇게 생성된 a 행렬에 SVD를 적용해 U, sigma, Vt를 도출하겠습니다. SVD 분해는 nummpy. linalg. svd에 파라미터로 원본 행렬을 입혁하면 U 행렬, Sigma 행렬, V 전치 행렬을 반환합니다. Sigma 행렬의 경우, S=UΣV 에서 Σ행렬의 경우 행렬의 대각에 위치한 값만 0이 아니고, 그렇지 않은 경우는 모두 0이므로 0이아닌 값의 경우만 1차원 행렬로 표현합니다.


```python
U, Sigma, Vt = svd(a)
print(U.shape, Sigma.shape, Vt.shape)
print('U matrix:\n', np.round(U, 3))
print('Sigma value:\n', np.round(Sigma, 3))
print('V transpose matrix:\n', np.round(Vt, 3))
```

    (4, 4) (4,) (4, 4)
    U matrix:
     [[-0.079 -0.318  0.867  0.376]
     [ 0.383  0.787  0.12   0.469]
     [ 0.656  0.022  0.357 -0.664]
     [ 0.645 -0.529 -0.328  0.444]]
    Sigma value:
     [3.423 2.023 0.463 0.079]
    V transpose matrix:
     [[ 0.041  0.224  0.786 -0.574]
     [-0.2    0.562  0.37   0.712]
     [-0.778  0.395 -0.333 -0.357]
     [-0.593 -0.692  0.366  0.189]]
    

U 행렬이 4 X , Vt행렬이 4 X 4로 반환됐고, Sigma의 경우는 1차원 행렬인 (4,)로 반환됐습니다.

분해된 이 U, Sigma, Vt를 이용해 다시 원본 행렬로 정확히 복원되는지 확인해 보겠습니다. 원본 행렬로의 복원은 이 U, Sigma, Vt를 내적하면 됩니다. 한 가지 유의할 것은 Sigma의 경우 0이 아닌 값만 1차원으로 추출했으므로 다시 0을 포함한 대칭 행렬로 변환한 뒤에 내적을 수행해야 한다는 점입니다.


```python
# Sigma를 다시 0을 포함한 대칭행렬로 변환
Sigma_mat = np.diag(Sigma)
a_ = np.dot(np.dot(U, Sigma_mat), Vt)
print(np.round(a_, 3))
```

    [[-0.212 -0.285 -0.574 -0.44 ]
     [-0.33   1.184  1.615  0.367]
     [-0.014  0.63   1.71  -1.327]
     [ 0.402 -0.191  1.404 -1.969]]
    

U, Sigma, Vt를 이용해 a_는 원본 행렬 a와 동일하게 복원됨을 알 수 있습니다. 이번에는 데이터 세트가 로우 간 의존성이 있을 경우 어떻게 Sigma 값이 변하고, 이에 따른 차원 축소가 진행될 수 있는지 알아보겠습니다. 일부러 의존성을 부여하기 위해 a 행렬의 3번째 로우를 '첫 번째 로우 + 두 번째 로우'로 업데이트하고, 4번째 로우는 첫 번째 로우와 같다고 업데이트 하겠습니다.


```python
a[2] = a[0] + a[1]
a[3] = a[0]
print(np.round(a, 3))
```

    [[-0.212 -0.285 -0.574 -0.44 ]
     [-0.33   1.184  1.615  0.367]
     [-0.542  0.899  1.041 -0.073]
     [-0.212 -0.285 -0.574 -0.44 ]]
    
